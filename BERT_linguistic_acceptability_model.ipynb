{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrtanair/master_thesis/blob/main/BERT_linguistic_acceptability_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "lPdMa_esvWvV",
        "outputId": "086d135b-78e1-42c5-ee94-d058801fad3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.4-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.10.0-py2.py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.1/302.1 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.10.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Using cached wget-3.2-py3-none-any.whl\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Downloading dataset...\n",
            "Archive:  cola_public_1.1.zip\n",
            "   creating: cola_public/\n",
            "  inflating: cola_public/README      \n",
            "   creating: cola_public/tokenized/\n",
            "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
            "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
            "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
            "   creating: cola_public/raw/\n",
            "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
            "  inflating: cola_public/raw/in_domain_train.tsv  \n",
            "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "all_datasets = [ \"CoLA\", \"MegaAcceptability\"]\n",
        "dataset = all_datasets[0]\n",
        "debug = False\n",
        "model_save = True\n",
        "model_name = 'bert-large-uncased'\n",
        "\n",
        "if not debug:\n",
        "  !pip install wandb\n",
        "  import wandb\n",
        "  wandb.login()\n",
        "\n",
        "!pip install wget\n",
        "import wget\n",
        "import os\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import json\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "if dataset == \"CoLA\":\n",
        "  url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "  if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    print('Downloading dataset...')\n",
        "    wget.download(url, './cola_public_1.1.zip')\n",
        "  if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip\n",
        "\n",
        "  train_path = '/content/cola_public/raw/in_domain_train.tsv'\n",
        "  dev_path = '/content/cola_public/raw/in_domain_dev.tsv'\n",
        "  test_path = '/content/cola_public/raw/out_of_domain_dev.tsv'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSXsDvisx09p"
      },
      "outputs": [],
      "source": [
        "class AcceptabilityDataset():\n",
        "  def __init__(self, texts, labels, length):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.max_length = length\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    self.n_examples = len(labels)\n",
        "    return\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_examples\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.texts[idx]\n",
        "    label = self.labels[idx]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_length,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'labels': torch.tensor(label, dtype=torch.long)\n",
        "    }\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "  def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "    super(FocalLoss, self).__init__()\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "    self.reduction = reduction\n",
        "\n",
        "  def forward(self, inputs, targets):\n",
        "    inputs_cpu = inputs.cpu()\n",
        "    y_targets = torch.zeros(inputs.shape[0], 2)\n",
        "    y_targets[range(y_targets.shape[0]), targets]=1\n",
        "\n",
        "    p = torch.sigmoid(inputs_cpu)\n",
        "    ce_loss = F.binary_cross_entropy_with_logits(inputs_cpu, y_targets, reduction=\"none\")\n",
        "    p_t = p * y_targets + (1 - p) * (1 - y_targets)\n",
        "    focal_loss = ce_loss * ((1 - p_t) ** self.gamma)\n",
        "\n",
        "    if self.reduction == 'mean':\n",
        "        return focal_loss.mean()\n",
        "    elif self.reduction == 'sum':\n",
        "        return focal_loss.sum()\n",
        "    else:\n",
        "        return focal_loss\n",
        "\n",
        "def train(model, dataloader, optimizer, device, scheduler):\n",
        "  criterion = FocalLoss()\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  total_correct = 0\n",
        "  total_samples = 0\n",
        "\n",
        "  for batch in dataloader:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    loss = criterion(logits, labels)\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=1)\n",
        "    total_correct += (predictions == labels).sum().item()\n",
        "    total_samples += labels.size(0)\n",
        "\n",
        "  avg_loss = total_loss / len(dataloader)\n",
        "  accuracy = total_correct / total_samples\n",
        "  return model, avg_loss, accuracy\n",
        "\n",
        "def validate(model, dataloader, device):\n",
        "  criterion = FocalLoss()\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  total_correct = 0\n",
        "  total_samples = 0\n",
        "\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "      logits = outputs.logits\n",
        "      loss = criterion(logits, labels)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      predictions = torch.argmax(logits, dim=1)\n",
        "      all_preds.extend(predictions.cpu().numpy())\n",
        "      all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "      total_correct += (predictions == labels).sum().item()\n",
        "      total_samples += labels.size(0)\n",
        "\n",
        "  avg_loss = total_loss / len(dataloader)\n",
        "  accuracy = total_correct / total_samples\n",
        "  return avg_loss, accuracy, all_preds, all_labels\n",
        "\n",
        "def save_model(model, path):\n",
        "\tmodel.save_pretrained(path)\n",
        "\n",
        "def load_model(path):\n",
        "\treturn BertForSequenceClassification.from_pretrained(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYtWAp81jM6_"
      },
      "outputs": [],
      "source": [
        "def run():\n",
        "  if not debug:\n",
        "    wandb.init()\n",
        "    learning_rate = wandb.config.learning_rate\n",
        "    batch_size = wandb.config.batch_size\n",
        "    length = wandb.config.length\n",
        "    optimizer = wandb.config.optimizer\n",
        "    weight_decay = wandb.config.weight_decay\n",
        "    hidden_dropout_prob = wandb.config.hidden_dropout_prob\n",
        "  else:\n",
        "    learning_rate = 5e-05\n",
        "    batch_size = 16\n",
        "    length = 32\n",
        "    optimizer = 'AdamW'\n",
        "    weight_decay = 0.2\n",
        "    hidden_dropout_prob = 0.1\n",
        "\n",
        "  seed = 42\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    device = torch.device(\"cuda\")\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    print('GPU:', device_name)\n",
        "  else:\n",
        "    print('Using CPU')\n",
        "    device = torch.device(\"cpu\")\n",
        "    device_name = 'cpu'\n",
        "\n",
        "  if dataset == \"CoLA\":\n",
        "    train_df = pd.read_csv(train_path, delimiter='\\t', header=None,\n",
        "                    names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "    val_df = pd.read_csv(dev_path, delimiter='\\t', header=None,\n",
        "                    names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "    test_df = pd.read_csv(test_path, delimiter='\\t', header=None,\n",
        "                    names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "  elif dataset in [\"MegaAcceptability\"]:\n",
        "    path = \"/content/drive/MyDrive/thesis/mega_acceptability.tsv\"\n",
        "    df = pd.read_csv(path, delimiter='\\t', header=None,\n",
        "                    names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    test_df, val_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "  texts = train_df.sentence.values\n",
        "  labels = train_df.label.values\n",
        "  train_dataset = AcceptabilityDataset(texts, labels, length)\n",
        "  train_dataloader = DataLoader(train_dataset,\n",
        "                                sampler = RandomSampler(train_dataset),\n",
        "                                batch_size = batch_size)\n",
        "\n",
        "  val_texts = val_df.sentence.values\n",
        "  val_labels = val_df.label.values\n",
        "  val_dataset = AcceptabilityDataset(val_texts, val_labels, length)\n",
        "  val_dataloader = DataLoader(val_dataset,\n",
        "                                sampler = SequentialSampler(val_dataset),\n",
        "                                batch_size = batch_size)\n",
        "\n",
        "  test_texts = test_df.sentence.values\n",
        "  test_labels = test_df.label.values\n",
        "  test_dataset = AcceptabilityDataset(test_texts, test_labels, length)\n",
        "  test_dataloader = DataLoader(test_dataset,\n",
        "                                sampler = SequentialSampler(test_dataset),\n",
        "                                batch_size = batch_size)\n",
        "\n",
        "  model = BertForSequenceClassification.from_pretrained(model_name,\n",
        "                                                        num_labels = 2,\n",
        "                                                        hidden_dropout_prob = hidden_dropout_prob)\n",
        "  model.to(device)\n",
        "\n",
        "  if optimizer == 'AdamW':\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
        "  elif optimizer == 'Adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
        "\n",
        "  total_steps = len(train_dataloader) * 2\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                              num_warmup_steps = 10000,\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "  all_loss = {'train_loss':[], 'val_loss':[]}\n",
        "  all_acc = {'train_acc':[], 'val_acc':[]}\n",
        "\n",
        "  curr_val_loss = None\n",
        "  prev_val_loss = None\n",
        "  epoch = 1\n",
        "\n",
        "  #dynamic early stopping based on validation loss\n",
        "  while True:\n",
        "    print(\"Epoch: \", epoch)\n",
        "    print('Training...')\n",
        "    train_dataloader_tqdm = tqdm(train_dataloader, desc=f\"Training Epoch {epoch}\")\n",
        "    model, train_loss, train_accuracy = train(model, train_dataloader_tqdm, optimizer, device, scheduler)\n",
        "\n",
        "    print('Validation...')\n",
        "    val_loss, val_accuracy, val_predict, val_labels = validate(model, val_dataloader, device)\n",
        "    prev_val_loss = curr_val_loss\n",
        "    curr_val_loss = val_loss\n",
        "\n",
        "    print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_accuracy, val_accuracy))\n",
        "    all_loss['train_loss'].append(train_loss)\n",
        "    all_loss['val_loss'].append(val_loss)\n",
        "    all_acc['train_acc'].append(train_accuracy)\n",
        "    all_acc['val_acc'].append(val_accuracy)\n",
        "\n",
        "    if curr_val_loss is not None and prev_val_loss is not None:\n",
        "      if curr_val_loss > prev_val_loss or curr_val_loss == prev_val_loss:\n",
        "        print(f'Early stopping due to no improvement in validation loss.')\n",
        "        break\n",
        "      if epoch > 4:\n",
        "        print(f'Early stopping due to maximum number of epochs exceeded(maximum is 5).')\n",
        "        break\n",
        "    epoch = epoch + 1\n",
        "\n",
        "  mcc = matthews_corrcoef(val_labels, val_predict)\n",
        "  print(f'Final MCC is {mcc} for dev data')\n",
        "\n",
        "  if not debug:\n",
        "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss,\n",
        "              \"val_loss\": val_loss, \"train_acc\": train_accuracy,\n",
        "              \"val_acc\": val_accuracy, \"MCC\": mcc})\n",
        "  else:\n",
        "    _, test_accuracy, test_predict, test_labels = validate(model, test_dataloader, device)\n",
        "    test_mcc = matthews_corrcoef(test_labels, test_predict)\n",
        "    print(f'Final MCC is {test_mcc} for test data')\n",
        "    print(f'Difference in MCC is: ', abs(mcc-test_mcc))\n",
        "\n",
        "    if model_save:\n",
        "      output_dir =  './' + dataset + '-' + model_name + '/'\n",
        "      training_args = {'created': datetime.datetime.now().strftime('%d_%m_%Y_%H_%M_%S'),\n",
        "              'dataset': dataset,\n",
        "              'model': model_name,\n",
        "              'device': device_name,\n",
        "              'batch_size': batch_size,\n",
        "              'epochs': epoch,\n",
        "              'learning_rate': learning_rate,\n",
        "              'optimizer': type(optimizer).__name__,\n",
        "              'seed': seed,\n",
        "              'test_MCC': test_mcc,\n",
        "              'dev_MCC': mcc,\n",
        "              'accuracy': test_accuracy,\n",
        "              'weight_decay': weight_decay,\n",
        "              'hidden_dropout_prob': hidden_dropout_prob,\n",
        "              'length': length,\n",
        "              }\n",
        "      os.makedirs(output_dir)\n",
        "      print(\"Saving model to: \", output_dir)\n",
        "      if os.path.exists(output_dir):\n",
        "        shutil.rmtree(output_dir)\n",
        "      save_model(model, output_dir)\n",
        "\n",
        "      with open(output_dir + '/training_args.json', \"w\") as json_file:\n",
        "        json.dump(training_args, json_file)\n",
        "      with open(output_dir + '/all_loss.json', \"w\") as json_file:\n",
        "        json.dump(all_loss, json_file)\n",
        "      with open(output_dir + '/all_acc.json', \"w\") as json_file:\n",
        "          json.dump(all_acc, json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf7micvydv0f",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  if debug:\n",
        "    run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if debug and model_save:\n",
        "  !cp -r /content/MegaAcceptability-bert-large-uncased /content/drive/MyDrive/thesis/models/MegaAcceptability-bert-large-uncased"
      ],
      "metadata": {
        "id": "PnKwUIYL427p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "raIYIBxLAgoh"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  sweep_configuration = {\n",
        "      \"method\": \"bayes\",\n",
        "      \"name\": \"sweep\",\n",
        "      \"metric\": {\"goal\": \"maximize\", \"name\": \"MCC\"},\n",
        "      \"parameters\": {\n",
        "          \"optimizer\": {\"values\": ['Adam', 'AdamW']},\n",
        "          \"batch_size\": {\"values\": [16, 32]},\n",
        "          \"learning_rate\": {\"values\": [2e-5, 3e-5, 5e-5]},\n",
        "          \"length\": {\"values\": [32, 64]},\n",
        "          \"weight_decay\": {\"min\": 0.01, \"max\": 0.3},\n",
        "          \"hidden_dropout_prob\": {\"min\": 0.01, \"max\": 0.3},\n",
        "      },\n",
        "  }\n",
        "  project_name = dataset + \"-\" + model_name\n",
        "  if not debug:\n",
        "    sweep_id = wandb.sweep(sweep = sweep_configuration, project = project_name)\n",
        "    wandb.agent(sweep_id, function = run, count = 20)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN7OVT3Yq7xk1a7JHZinL+8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}